# AI Collaboration Documentation

Disclaimer: my usual approach is different. When I code, I usually just write what I want, and occasionally accept the AI plugin’s (most likely Windsurf’s) suggestions about what should follow. And then when I’m not familiar with how to do something I ask a publicly available LLM (most likely Gemini or Claude) as if I were asking a colleague during pair programming. But as I didn’t find a way to document these autocompletions, as I was required to document all of my AI interactions, I decided to go with fully vibe coding the solution for this case.

## Key AI Interactions

- **Architectural decision**: I was solving two problems, I needed to adhere to the technical requirement of a fast microservice, and I also needed an architecture that I think any AI would be comfortable to work with. For this I created a custom chat (`MiklosBalazs_Gemini2-5FlashApi_conversation_1.md`) where I used a system prompt to make the LLM act as a software architect. In the regard of frameworks it offered me Spring Boot or Quarkus or Micronaut, but as it pointed out Spring Boot can consume a lot of resources, so for a more scalable microservice implementation I went with Quarkus. From the architectural point of view it offered me Hexagonal Architecture and Layered Architecture. Here I chose Hexagonal, because I know it to be more restrictive, with better separation of concerns, so I thought that working with just specific parts of the project at a time would be beneficial as the LLMs would need a smaller context for their tasks. For this reason I also modularized the project early on.
- **Implementation challenge**: Probably the biggest challenge I faced was the time that one of the LLMs hallucinated a bit too many dependencies into the project. As I never worked with Quarkus before, didn’t realize that one of the libraries were for Quarkus 2 while the project used Quarkus 3, and the conflict caused quite persistent build problems, that agentic tools like Windsurf and Gemini CLI failed to analyze properly, running in circles. Here my solution was to start a chat with a custom system prompt (`MiklosBalazs_Gemini2-5FlashApi_conversation_10.md`) where the instructions were to help me construct a Stack Overflow question that adheres to their standards of a good question. This results in some kind of interactive rubber duck debugging session, where by the end the AI usually finds the solution. Here it was the same, the LLM pointed out that one of my dependencies is not for this version of Quarkus. And in the worst case if we didn’t find the problem, then we would still have come up with a question that I could have asked on Stack Overflow.
- **Critical evaluation**: as pointed out before partly could not be done, but mostly was not necessary. I had to pay attention mostly to the cases where the LLM claimed that something was done and wanted to jump ahead to the next task (e.g. `MiklosBalazs_Gemini-Cli_conversation_4.md` line 43) or just didn’t care about the state of the project at all. Also LLMs can have strange ideas about the tooling of the project, e.g. some times (`MiklosBalazs_Windsurf_conversation_15.md` line 319) it wanted to run the build with the `--stacktrace` flag, when Gradle produces additional stack traces, which extends the LLM context enough to completely confuse the agent. I think in these cases only past experience can help, so I’m glad that I went with Gradle over Maven, as I’m more familiar with that technology.

## Validation and Learning

- **Validation methods**: I think the first line of defense is reviewing the code. I found that easier with Gemini CLI, Windsurf not showing its changes made this virtually impossible. Other than this I realized that tests are more important than ever. There was even a case (`MiklosBalazs_Windsurf_conversation_13.md` line 445) where a leftover integration test that was no longer relevant from the business point of view, was necessary to catch an issue with a refactoring of the LLM. Also this is a good example to be strict with the LLM: it tried to play it off as the failure of this test has nothing to do with its changes. Another time I wanted it to fix warnings, which clearly pointed to issues in the mapping logic, but I was told that those warnings are irrelevant. (`MiklosBalazs_Windsurf_conversation_13.md` line 523. Sadly the most relevant part was lost in my sloppy copy-pasting.)
- **Context provision**: Usually not needed. Agentic AI is quite proactive in looking up information. I counted (`rg "\*\*Read" | rg "\*\*" | wc`) 42 occurrences in my Gemini CLI logs (not available in the Windsurf logs, because of copy-pasting difficulties.) where it looked up file contents, mostly without prompting. But referencing files directly as a starting point for an operation (`MiklosBalazs_Windsurf_conversation_15.md` line 124) is also a handy feature of Windsurf/Cursor.
- **Key insight**: Previously I was very sceptical of the utilization of generative AI, especially to this extent. However, apparently LLMs can be very productive in this regard. But knowing that it spent hours on an unrestricted Hello World application to modularize it, I realize that it needs as many restrictions as possible. In a production environment, I’d probably add even more… linting, Sonar, PMD, SpotBug, harder test coverage limits, maybe even some manual rules.


